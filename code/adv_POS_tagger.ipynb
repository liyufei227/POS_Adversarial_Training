{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following link is useful for understanding sampler, batching, and sequence padding work.\n",
    "    https://www.scottcondron.com/jupyter/visualisation/audio/2020/12/02/dataloaders-samplers-collate.html#Custom-Sampler\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.sampler import BatchSampler, Sampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import random\n",
    "\n",
    "SEED=4321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "class Corpora():\n",
    "    \"\"\"\n",
    "    The class holds training and test corpora.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \"\"\"\n",
    "        # word to index (1-based integers) mapping\n",
    "        self.word_index = {}\n",
    "        # POS-tag to index (1-based integers) mapping\n",
    "        self.tag_index = {}\n",
    "        # index to POS-tag mapping: the reverse mapping of the above\n",
    "        self.index_tag = {}\n",
    "        # list of sentences, each of which is a list of pairs of integer indices (word_index[w_t], tag_index[tag_t]),\n",
    "        # where w_t and tag_t are the word and POS tag at the location t of a sentence, respectively.\n",
    "        self.training_sentences = []\n",
    "        # list of sentences. Same format as training_sentences\n",
    "        self.test_sentences = []\n",
    "\n",
    "        self.max_len = 0\n",
    "\n",
    "    def read_corpus(self, corpus_path, is_training):\n",
    "        \"\"\"\n",
    "        Read a corpus. It is important that you let the words in the training and test corpora share the same index,\n",
    "            so that there is no unseen words in the test set.\n",
    "            Make sure that the indices are 1-based as 0 is reserved for padding.\n",
    "        :param corpus_path: path to a file with POS-tagged sentences.\n",
    "        :param is_training: if true, the file is for the training corpus, otherwise the test corpus\n",
    "        :return: nothing.\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "        with open(corpus_path, 'r') as f:\n",
    "            ### Your codes go here (10 points) ###\n",
    "            sentence = []\n",
    "            for line in f:\n",
    "                #tokens = line.strip().split()\n",
    "                #tokens = [token.lower() for token in tokens]  #lowercase\n",
    "                if line != \"\" and line != \"\\n\":\n",
    "                    (word, tag, _) = line.split(' ')\n",
    "                    word = word.lower()\n",
    "                    word_index = self.word_index.get(word)\n",
    "                    if word_index == None:\n",
    "                        self.word_index[word] = len(self.word_index)\n",
    "                        word_index = self.word_index[word]\n",
    "                    tag_index = self.tag_index.get(tag)\n",
    "                    if tag_index == None:\n",
    "                        self.tag_index[tag] = len(self.tag_index)\n",
    "                        ### added code\n",
    "                        self.index_tag[len(self.tag_index)] = tag\n",
    "                        tag_index = self.tag_index[tag]\n",
    "                        \n",
    "                    sentence.append((word_index, tag_index))\n",
    "                    \n",
    "                else:\n",
    "                    sentences.append(sentence)\n",
    "                    self.max_len = max(self.max_len, len(sentence))\n",
    "                    sentence = []\n",
    "                    \n",
    "        if is_training:\n",
    "            self.training_sentences = sentences\n",
    "        else:\n",
    "            self.test_sentences = sentences\n",
    "                        \n",
    "                        \n",
    "                    \n",
    "\n",
    "                \n",
    "class POSTaggedDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Define a POS-tagged sentence dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_pairs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.sequence_pairs = sequence_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence, tags = zip(*self.sequence_pairs[idx])\n",
    "        return torch.tensor(sentence), torch.tensor(tags)\n",
    "\n",
    "    \n",
    "class SortedBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "        Each sequence in a mini-batch must of the same lengths, while our sentences\n",
    "        are of various lengths.\n",
    "        We can pad the sentences to the same lengths in each mini-batch.\n",
    "        But if a short and long sentences are in the same mini-batch, more paddings\n",
    "        are needed.\n",
    "        We sort the sentences based on their lengths (in descending order)\n",
    "            and then put sentences with similar lengths in a batch to reduce the paddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        \"\"\"\n",
    "            dataset: an torch.utils.data.DataSet object containing all training sequences\n",
    "            batch_size: the number of sequences to put in a mini-batch\n",
    "        \"\"\"\n",
    "        ### Your codes go here (5 points) ###\n",
    "        # The sorting and batching go within this function.\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Compute the length of each sequence in the dataset\n",
    "        self.lengths = [len(sequence[0]) for sequence in dataset]\n",
    "\n",
    "        # Create a list of indices that represent the order in which the sequences should be sorted\n",
    "        self.indices = sorted(range(len(self.lengths)), key=lambda x: self.lengths[x])\n",
    "        self.sorted_lengths = len(self.indices)\n",
    "        \n",
    "        # Split the sorted indices into batches of size batch_size\n",
    "        self.index_batches = [self.indices[i:i+self.batch_size] for i in range(0, len(self.indices), self.batch_size)]\n",
    "        \n",
    "        \n",
    " \n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "            return a Python iterator object that iterates the mini-batchs of\n",
    "                training data indices (not individual indices)\n",
    "        \"\"\"\n",
    "        return iter(self.index_batches)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sorted_lengths // self.batch_size\n",
    "\n",
    "def padding_collate_func(batch):\n",
    "    \"\"\"\n",
    "        Transform pairs of input-output sequences in the batch to be of the same length using the function\n",
    "            torch.nn.utils.rnn.pad_sequence.\n",
    "        batch: An iterator and each element is a pair of (input_sequence, output_sequence).\n",
    "        For POS tagging, len(input_sequence) = len(output_sequence). But for different\n",
    "        pairs in batch, their lengths can differ.\n",
    "\n",
    "        Example: a batch of 3 pairs of input/output sequences\n",
    "                [([1,2,3],[1,1,1]), ([1,2,3,4],[2,2,2,2]), ([1,2,3,4,5],[3,3,3,3,3])]\n",
    "                Note: [] encloses tensors (not numpy arrays)\n",
    "        return: two tensors (one for input sequence batch and another for output sequence batch).\n",
    "                These tensors are padded with zeros so that all sequences in the same batch\n",
    "                are of the same length.\n",
    "        Example: input_sequence_batch = [[1,2,3,0,0], [1,2,3,4,0], [1,2,3,4,5]],\n",
    "                 output_sequence_batch = [[1,1,1,0,0], [2,2,2,2,0], [3,3,3,3,3]]\n",
    "\n",
    "    \"\"\"\n",
    "    ### Your codes go here (5 points) ###\n",
    "    # Hint: read the article linked at the top of this cell.\n",
    "    # Get the input and output sequences from the batch\n",
    "    input_seqs = [item[0] for item in batch]\n",
    "    output_seqs = [item[1] for item in batch]\n",
    "\n",
    "    # Pad the input and output sequences\n",
    "    input_padded = pad_sequence(input_seqs, batch_first=True, padding_value=0)\n",
    "    output_padded = pad_sequence(output_seqs, batch_first=True, padding_value=0)\n",
    "\n",
    "    return input_padded, output_padded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LSTMPOSTagger(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, emb_dim, hid_dim, n_layers, dropout, bidirectional):\n",
    "        \"\"\"\n",
    "        :param input_dim: size of the vocabulary (number of unique tokens)\n",
    "        :param output_dim: number of unique POS tags \n",
    "        :param emb_dim: embedding dimensionality of each token\n",
    "        :param hid_dim: number of hidden neurons of a hidden state/cell\n",
    "        :param n_layers: number of RNN layers (2 for faster training)\n",
    "        :param dropout: dropout rate between 0 and 1at the embedding layer and rnn\n",
    "        :param bidirectional: 1 if use bidirectional and 0 if don't\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "\n",
    "        # before output, there is a dropout (except the last layer)\n",
    "        if bidirectional == 0:\n",
    "            self.rnn = nn.LSTM(input_size = emb_dim, hidden_size = hid_dim, num_layers = n_layers, dropout=dropout)\n",
    "            self.fc = nn.Linear(hid_dim, output_dim)\n",
    "            self.num_directions = 1\n",
    "        elif bidirectional == 1:\n",
    "            self.rnn = nn.LSTM(input_size = emb_dim, hidden_size = hid_dim, num_layers = n_layers, dropout=dropout, bidirectional=True)\n",
    "            self.fc = nn.Linear(hid_dim * 2, output_dim)\n",
    "            self.num_directions = 2\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "\n",
    "        :param src: a [batch_size, sentence_len] array.\n",
    "                     Each row is a sequence of word indices and each column represents a position in the sequence.\n",
    "        :return: the predicted logits at each position. \n",
    "        \"\"\"\n",
    "        ### Your codes go here (20 points) ###\n",
    "\n",
    "        # Step 1: turn token indices into dense vector,\n",
    "        # so that embedded is of shape (batch_size, sentence_len, emb_dim)\n",
    "        embedded = self.dropout(self.embedding(src)) \n",
    "\n",
    "        # Step 2: rnn maps the tensor (batch_size, sentence_len, emb_dim) to\n",
    "        # outputs = a tensor (batch_size, sentence_len, hid_dim)\n",
    "        # hidden = a tensor (batch_size, sentence_len, hid_dim)\n",
    "        # cell = a tensor (batch_size, sentence_len, hid_dim)\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "        # Step 3: map the output tensor to a logit tensor of shape (batch_size, sentence_len, number_of_POS_tags)\n",
    "        logits = self.fc(self.dropout(outputs))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences = 8936\n",
      "Number of test sentences = 2012\n",
      "Number of unique input tokens = 19460\n",
      "Number of POS tags = 44\n",
      "Maximal sentence length = 78\n",
      "Epoch: 01 | Time: 0m 6s\tTrain Loss: 38.303 | Test Loss: 17.573\n",
      "Epoch: 02 | Time: 0m 3s\tTrain Loss: 10.328 | Test Loss: 7.212\n",
      "Epoch: 03 | Time: 0m 3s\tTrain Loss: 5.248 | Test Loss: 6.295\n",
      "Epoch: 04 | Time: 0m 3s\tTrain Loss: 3.722 | Test Loss: 6.016\n",
      "Epoch: 05 | Time: 0m 3s\tTrain Loss: 3.039 | Test Loss: 6.217\n",
      "Epoch: 06 | Time: 0m 3s\tTrain Loss: 2.658 | Test Loss: 6.228\n",
      "Epoch: 07 | Time: 0m 3s\tTrain Loss: 2.361 | Test Loss: 6.439\n",
      "Epoch: 08 | Time: 0m 3s\tTrain Loss: 2.116 | Test Loss: 6.731\n",
      "Epoch: 09 | Time: 0m 3s\tTrain Loss: 1.870 | Test Loss: 7.080\n",
      "Epoch: 10 | Time: 0m 3s\tTrain Loss: 1.643 | Test Loss: 7.318\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    num_batchs = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        num_batchs += 1\n",
    "\n",
    "        ### Your codes go here (5 points) ###\n",
    "        input_batch, target_batch = batch\n",
    "\n",
    "        input_batch = input_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_batch).to(device)\n",
    "\n",
    "        loss = criterion(logits.view(-1, logits.shape[-1]), target_batch.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Clips gradient norm of an iterable of parameters.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_pairs += len(target_batch)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / total_pairs\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    num_epochs = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        num_epochs += 1\n",
    "\n",
    "        ### Your codes go here (5 points) ###\n",
    "        input_batch, target_batch = batch\n",
    "\n",
    "        input_batch = input_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_batch)\n",
    "\n",
    "        loss = criterion(logits.view(-1, logits.shape[-1]), target_batch.view(-1))\n",
    "        \n",
    "        total_pairs += len(target_batch)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / total_pairs \n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "#training_path = '../data-badassnlp/project_2_data/train.txt'\n",
    "training_path = '/data/badassnlp/project_2_data/train.txt'\n",
    "#test_path = '../data-badassnlp/project_2_data/test.txt'\n",
    "test_path = '/data/badassnlp/project_2_data/test.txt'\n",
    "\n",
    "corpora = Corpora()\n",
    "\n",
    "corpora.read_corpus(training_path, is_training=True)\n",
    "corpora.read_corpus(test_path, is_training=False)\n",
    "\n",
    "print(f'Number of training sentences = {len(corpora.training_sentences)}')\n",
    "print(f'Number of test sentences = {len(corpora.test_sentences)}')\n",
    "print(f'Number of unique input tokens = {len(corpora.word_index)}')\n",
    "print(f'Number of POS tags = {len(corpora.tag_index)}')\n",
    "print(f'Maximal sentence length = {corpora.max_len}')\n",
    "\n",
    "training_dataset = POSTaggedDataset(corpora.training_sentences)\n",
    "#print (training_dataset[0])\n",
    "training_sampler = SortedBatchSampler(training_dataset, batch_size=BATCH_SIZE)\n",
    "#print (training_sampler[0])\n",
    "training_iterator = DataLoader(training_dataset,\n",
    "                                  collate_fn = padding_collate_func,\n",
    "                                  batch_sampler = training_sampler)\n",
    "#print (training_iterator[0])\n",
    "\n",
    "test_dataset = POSTaggedDataset(corpora.test_sentences)\n",
    "test_sampler = SortedBatchSampler(test_dataset, batch_size=BATCH_SIZE)\n",
    "test_iterator = DataLoader(test_dataset,\n",
    "                              collate_fn = padding_collate_func,\n",
    "                              batch_sampler = test_sampler)\n",
    "\n",
    "INPUT_DIM = len(corpora.word_index)+1\n",
    "OUTPUT_DIM = len(corpora.tag_index)+1\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2 # number of LSTM layers.\n",
    "BIDIRECT = 1 # 0: single direction (the default setting); 1: bidirectional\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# initialize the model\n",
    "POSTagger = LSTMPOSTagger(INPUT_DIM, OUTPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT, BIDIRECT).to(device)\n",
    "\n",
    "def init_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "POSTagger.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(POSTagger.parameters())\n",
    "\n",
    "# we use 0 to represent padded POS tags and the loss function should ignore that.\n",
    "# we calculate the sum of losses of pairs in each batch\n",
    "PAD_INDEX = 0\n",
    "criterion = nn.CrossEntropyLoss(reduction = 'sum', ignore_index = PAD_INDEX)\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "training_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    training_loss = train(POSTagger, training_iterator, optimizer, criterion, CLIP)\n",
    "    training_losses.append(training_loss)\n",
    "    \n",
    "    test_loss = evaluate(POSTagger, test_iterator, criterion)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss \n",
    "        torch.save(POSTagger.state_dict(), 'best_model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s', end='')\n",
    "    print(f'\\tTrain Loss: {training_loss:.3f} | Test Loss: {test_loss:.3f}')\n",
    "\n",
    "import pickle\n",
    "with open(f'results/losses_L{N_LAYERS}_D{DROPOUT}_B{BIDIRECT}.pkl', 'wb') as f:\n",
    "    pickle.dump({'training_losses': training_losses,\n",
    "                'test_losses': test_losses}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "x = np.arange(len(training_losses))\n",
    "\n",
    "plt.plot(x, training_losses, label = 'training loss')\n",
    "plt.plot(x, test_losses, label = 'test loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model to generate adversarial embedding for testing\n",
    "\n",
    "class AdvEmbeddingGenerator(LSTMPOSTagger):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, emb_dim, hid_dim, n_layers, dropout, bidirectional):\n",
    "        \"\"\"\n",
    "        model(LSTMPOSTagger)\n",
    "        \"\"\"\n",
    "        super().__init__(input_dim, output_dim, emb_dim, hid_dim, n_layers, dropout, bidirectional)\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            #print (param)\n",
    "            param.requires_grad = False\n",
    "        #self.fc.requires_grad = False\n",
    "        #self.rnn.requires_grad = False\n",
    "        #self.embedding.requires_grad = False\n",
    "        self.advembedding = nn.Embedding.from_pretrained(self.embedding.weight)\n",
    "        self.advembedding.weight.requires_grad = True\n",
    "        \n",
    "    \n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        same as LSTMPOSTagger\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.advembedding(src)) \n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "        logits = self.fc(self.dropout(outputs))\n",
    "\n",
    "        return logits\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train adversarial embedidng generate model \n",
    "def train_adv_generate(model, iterator, optimizer, criterion, clip):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    num_batchs = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        num_batchs += 1\n",
    "\n",
    "        ### Your codes go here (5 points) ###\n",
    "        input_batch, target_batch = batch\n",
    "\n",
    "        input_batch = input_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_batch).to(device)\n",
    "        \n",
    "        #the loss is negative since we hope that the adversarial embedding as worse as possible,\n",
    "        #but we don't want the adversarial embedding too far from the original embedding, so we add L2 penalty to adjust.\n",
    "        loss = (- criterion(logits.view(-1, logits.shape[-1]), target_batch.view(-1))\n",
    "                + 1000*torch.norm(model.embedding(input_batch) - model.advembedding(input_batch), p=2)) #unconstrainted, penalty\n",
    "        \n",
    "        #if i == 0:\n",
    "        #    print (model.rnn.weight_ih_l0[0])\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Clips gradient norm of an iterable of parameters.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_pairs += len(target_batch)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / total_pairs\n",
    "\n",
    "#after training, project adversarial embedding back to l2 ball\n",
    "#eps = 0.1\n",
    "#model.advembedding = norm(model.advembedding - model.embedding)*eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 2s\tTrain Loss: -71.646 \n",
      "Epoch: 02 | Time: 0m 2s\tTrain Loss: -68.360 \n",
      "Epoch: 03 | Time: 0m 2s\tTrain Loss: -68.206 \n",
      "Epoch: 04 | Time: 0m 2s\tTrain Loss: -69.296 \n",
      "Epoch: 05 | Time: 0m 2s\tTrain Loss: -69.971 \n",
      "Epoch: 06 | Time: 0m 2s\tTrain Loss: -69.801 \n",
      "Epoch: 07 | Time: 0m 2s\tTrain Loss: -69.587 \n",
      "Epoch: 08 | Time: 0m 2s\tTrain Loss: -69.735 \n",
      "Epoch: 09 | Time: 0m 2s\tTrain Loss: -69.917 \n",
      "Epoch: 10 | Time: 0m 2s\tTrain Loss: -69.991 \n"
     ]
    }
   ],
   "source": [
    "training_sampler = SortedBatchSampler(training_dataset, batch_size=BATCH_SIZE)\n",
    "training_iterator = DataLoader(training_dataset,\n",
    "                                  collate_fn = padding_collate_func,\n",
    "                                  batch_sampler = training_sampler)\n",
    "\n",
    "adv_model = AdvEmbeddingGenerator(INPUT_DIM, OUTPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT, BIDIRECT).to(device)\n",
    "\n",
    "adv_optimizer = optim.Adam(adv_model.parameters())\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "adv_training_losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    adv_training_loss = train_adv_generate(adv_model, training_iterator, adv_optimizer, criterion, CLIP)\n",
    "    adv_training_losses.append(adv_training_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s', end='')\n",
    "    print(f'\\tTrain Loss: {adv_training_loss:.3f} ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.14586213189612\n"
     ]
    }
   ],
   "source": [
    "#use orginal model to evaluate adversarial embedding data\n",
    "\n",
    "test_sampler = SortedBatchSampler(test_dataset, batch_size=BATCH_SIZE)\n",
    "test_iterator = DataLoader(test_dataset,\n",
    "                              collate_fn = padding_collate_func,\n",
    "                              batch_sampler = test_sampler)\n",
    "\n",
    "test_loss = evaluate(adv_model, test_iterator, criterion)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0003, device='cuda:3', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.norm(adv_model.embedding.weight - adv_model.advembedding.weight, p=2)/len(adv_model.advembedding.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orginal model test lost for adversarial embedding data is around 77, whcih is much larger than the test lost for original embedding data which is around 7. (now adversarial embedding is only 0.0003 away from original embedding data in L2 norm in average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "def generate_adv_example(embedded, loss, perturb_scale):\n",
    "    # embedded: [n_examples, input_length, feature_dim]\n",
    "\n",
    "    grad = gradient.grad(loss, embedded)\n",
    "    grad = gradient.disconnected_grad(grad)\n",
    "\n",
    "    shifted = embedded + T.max(T.abs_(embedded))+1.0\n",
    "    grad_dim = (shifted/shifted).sum(axis=(1,2)).mean(axis=0) # grad dim for each example\n",
    "    sqrt_grad_dim = T.sqrt(grad_dim) # sqrt(input_length * emb_dim)\n",
    "    perturb = perturb_scale * sqrt_grad_dim * _scale_unit_l2(grad)\n",
    "\n",
    "    return embedded + perturb\n",
    "\n",
    "\n",
    "def adversarial_loss(ori_char_emb, ori_word_emb, loss_fn, loss=None, perturb_scale=0.02):\n",
    "    print '** perturb_scale =', perturb_scale, '**'\n",
    "\n",
    "    assert loss is not None\n",
    "    char_emb_adv = generate_adv_example(ori_char_emb, loss, perturb_scale)\n",
    "    word_emb_adv = generate_adv_example(ori_word_emb, loss, perturb_scale)\n",
    "\n",
    "    return loss_fn(char_emb_adv, word_emb_adv, return_all=False)\n",
    "    \n",
    "logger.info('Preparing adversarial training...')\n",
    "loss_train_adv = adversarial_loss(char_emb, word_emb, loss_from_embedding, loss_train_ori, perturb_scale=args.adv)\n",
    "loss_train = (loss_train_ori + loss_train_adv) / 2.0\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model to generate adversarial embedding for training\n",
    "def generate_batch_adv_embedding(model, input_batch, target_batch):\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters:\n",
    "        model: AdvPOSTAGGER w\n",
    "        input_batch(tensor(int)): index of the words in the sentences of the batch\n",
    "    \"\"\"\n",
    "    \n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    #optimizer.zero_grad()\n",
    "\n",
    "    logits = model(input_batch).to(device)\n",
    "\n",
    "    loss = criterion(logits.view(-1, logits.shape[-1]), target_batch.view(-1))\n",
    "    \n",
    "    loss.retain_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    #print (loss)\n",
    "    \n",
    "    #for name, param in model.named_parameters():\n",
    "    #    print (name)\n",
    "    #    print (param.grad)\n",
    "    \n",
    "    #print(model.embedding.weight.grad)\n",
    "    \n",
    "    return model.embedding.weight + 0.001*loss.grad\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdvTrainPOSTagger(LSTMPOSTagger):\n",
    "    def __init__(self, input_dim, output_dim, emb_dim, hid_dim, n_layers, dropout, bidirectional):\n",
    "        \"\"\"\n",
    "        model(LSTMPOSTagger)\n",
    "        \"\"\"\n",
    "        super().__init__(input_dim, output_dim, emb_dim, hid_dim, n_layers, dropout, bidirectional)\n",
    "        self.adv_embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        same as LSTMPOSTagger\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.adv_embedding(src)) \n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "        logits = self.fc(self.dropout(outputs))\n",
    "\n",
    "        return logits\n",
    "    \n",
    "\n",
    "def adv_train(model, iterator, optimizer, criterion, clip):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    num_batchs = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        num_batchs += 1\n",
    "\n",
    "        ### Your codes go here (5 points) ###\n",
    "        input_batch, target_batch = batch\n",
    "\n",
    "        input_batch = input_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "        \n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    \n",
    "        #model.adv_embedding.weight = torch.nn.parameter.Parameter(generate_batch_adv_embedding(model, input_batch, target_batch))\n",
    "\n",
    "        logits = model(input_batch).to(device)\n",
    "\n",
    "        loss = criterion(logits.view(-1, logits.shape[-1]), target_batch.view(-1))\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Clips gradient norm of an iterable of parameters.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_pairs += len(target_batch)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / total_pairs\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 3s\tTrain Loss: 77.364 \n",
      "Epoch: 02 | Time: 0m 3s\tTrain Loss: 77.366 \n",
      "Epoch: 03 | Time: 0m 3s\tTrain Loss: 77.364 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-1bf6b2ae12ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madv_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtraining_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-125-0a1f5b08df7d>\u001b[0m in \u001b[0;36madv_train\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m#model.adv_embedding.weight = torch.nn.parameter.Parameter(generate_batch_adv_embedding(model, input_batch, target_batch))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-125-0a1f5b08df7d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_sampler = SortedBatchSampler(training_dataset, batch_size=BATCH_SIZE)\n",
    "training_iterator = DataLoader(training_dataset,\n",
    "                                  collate_fn = padding_collate_func,\n",
    "                                  batch_sampler = training_sampler)\n",
    "\n",
    "advtrain_model = AdvTrainPOSTagger(INPUT_DIM, OUTPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT, BIDIRECT).to(device)\n",
    "\n",
    "training_losses = []\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    training_loss = adv_train(advtrain_model, training_iterator, optimizer, criterion, CLIP)\n",
    "    training_losses.append(training_loss)\n",
    "    \n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s', end='')\n",
    "    print(f'\\tTrain Loss: {training_loss:.3f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
